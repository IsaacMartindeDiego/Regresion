# Métodos de selección de variables y problemas de regularización {#sec-tema2}

En los modelos de regresión, especialmente cuando se trabaja con
conjuntos de datos que incluyen un gran número de variables predictoras,
es común enfrentarse al desafío de identificar qué variables son
realmente relevantes para explicar la variable respuesta. La inclusión
de demasiadas variables en un modelo puede llevar a problemas como el
sobreajuste, pérdida de interpretabilidad y complejidad innecesaria,
mientras que la exclusión de variables importantes puede resultar en
modelos subóptimos.

## Proceso de construcción del modelo de regresión

La construcción de un modelo de regresión múltiple es un proceso
sistemático que busca explicar la relación entre una variable respuesta
($Y$) y múltiples variables predictoras ($X_1, X_2, \dots, X_k$). Este
proceso consta de varias etapas clave [@kutner2005applied]:

1.  **Definición del problema y variables de interés:**
    -   Identificar claramente el objetivo del análisis, ya sea realizar
        predicciones, evaluar relaciones o controlar por efectos de
        variables confusoras.
    -   Seleccionar las variables predictoras potenciales en función de
        su relevancia teórica, conocimiento previo o exploración inicial
        de los datos.
2.  **Recogida de datos:**

-   La calidad de los datos recogidos influye directamente en la validez
    de los resultados y conclusiones obtenidas. El proceso de recogida
    de datos consiste en recopilar información de manera organizada y
    sistemática para responder a las preguntas de investigación
    planteadas. Dependiendo del diseño del estudio y los objetivos del
    análisis, se pueden emplear diferentes tipos de experimentos o
    métodos de recogida de datos.
-   Debemos asegurar las siguientes características sobre los datos.
    -   **Fiabilidad:** Asegurar que los datos sean consistentes y
        puedan reproducirse bajo condiciones similares.
    -   **Validez:** Garantizar que los datos recojan realmente la
        información necesaria para responder a las preguntas de
        investigación.
    -   **Ética:** Asegurar la privacidad y el consentimiento informado
        de los participantes.
    -   **Control de Sesgos:** Diseñar el estudio de manera que se
        minimicen los sesgos que puedan distorsionar los resultados.

::: {.callout-note collapse="true" title="Tipos de experimentos"}
La elección del tipo de experimento o método de recogida de datos
dependerá de la naturaleza del problema a investigar, los recursos
disponibles y las limitaciones del estudio. Una correcta planificación y
ejecución de esta etapa sienta las bases para un análisis robusto y
confiable.

1.  **Experimentos controlados:**
    -   Los experimentos controlados son diseñados de manera que los
        investigadores manipulan deliberadamente una o más variables
        independientes (llamadas factores o variables controladas) para observar su efecto en la variable
        dependiente.
    -   Incluyen la aleatorización de sujetos entre grupos (por ejemplo,
        grupos de control y tratamiento) para minimizar sesgos y
        asegurar comparabilidad.
    -   En muchas ocasiones la información suplementaria no se puede incorporar en el diseño del experimento. A esas variables, no controladas, se les suel llamar covariables.
    -   **Ejemplo:** Un estudio clínico donde se prueba un nuevo
        medicamento y se compara su efecto con un placebo.
2.  **Estudios observacionales exploratorios:**
    -   En este enfoque, los datos se recogen sin intervenir ni
        manipular las condiciones. Los investigadores observan y
        registran los fenómenos tal como ocurren en la naturaleza.
    -   Pueden clasificarse en:
        -   **Estudios transversales:** Los datos se recogen en un único
            punto temporal.
        -   **Estudios longitudinales:** Los datos se recogen durante un
            periodo para analizar cambios a lo largo del tiempo.
    -   **Ejemplo:** Investigar los hábitos alimenticios y su asociación
        con enfermedades cardiovasculares en una población.
3.  **Estudios observacionales confirmatorios:**
    -   En este enfoque, los datos se recogen para testear (confirmar o no) hipótesis derivadas de estudios previos o de ideas que pueden tener los investigadores.
    -   En este contexto, las variables que aparecen involucradas en la hipótesis que se quiere confirmar se denominan variables primarias, y las variables explicativas que se sabe inluyen en la respuesta se llaman variables de control (en Epidemiología nos referimos a ellas como factores de riesgo)
    -   **Ejemplo:** Un equipo de investigadores, basándose en estudios previos, plantea la hipótesis de que existe una relación positiva entre el hábito de fumar (variable explicativa principal) y la incidencia de cáncer de pulmón (variable respuesta). Para confirmar esta hipótesis, realizan un estudio observacional en el que recopilan datos de una población durante un periodo determinado. Dado que no es ético inducir a las personas a fumar para realizar un experimento controlado, este estudio se realiza de forma observacional. Los datos se analizan para evaluar la asociación entre las variables, permitiendo confirmar (o refutar) la hipótesis planteada con un diseño adecuado y controlando los posibles factores de confusión.
    
4.  **Encuestas y cuestionarios:**
    -   Las encuestas son una técnica común para recoger datos de manera
        estructurada sobre actitudes, opiniones, comportamientos o
        características demográficas.
    -   Pueden aplicarse en formato presencial, en línea, por teléfono o
        mediante correo.
    -   **Ejemplo:** Una encuesta para medir el grado de satisfacción de
        los clientes con un servicio.
5.  **Experimentos naturales:**
    -   Se producen cuando un fenómeno natural o social actúa como una
        intervención en un entorno sin que los investigadores tengan
        control sobre el experimento.
    -   Este tipo de estudio aprovecha eventos únicos para analizar sus
        impactos.
    -   **Ejemplo:** Estudiar los efectos económicos de una nueva
        política fiscal aplicada en una región específica.
6.  **Estudios de simulación:**
    -   Los datos se generan a través de modelos matemáticos o
        computacionales que representan un sistema real o hipotético.
    -   Este método se usa cuando es difícil o costoso realizar
        experimentos reales.
    -   **Ejemplo:** Simular el comportamiento de un mercado financiero
        bajo diferentes escenarios económicos.
7.  **Recogida de datos secundarios:**
    -   En lugar de recoger datos nuevos, se utilizan datos ya
        existentes recopilados por terceros, como censos, registros
        administrativos o bases de datos públicas.
    -   Aunque es eficiente en tiempo y costos, el investigador tiene
        menor control sobre la calidad y las características de los
        datos.
    -   **Ejemplo:** Analizar datos de encuestas nacionales para
        estudiar tendencias sociales.
:::

3.  **Análisis Exploratorio de Datos (EDA):**
    -   Inspeccionar los datos mediante análisis descriptivo y visual
        para identificar posibles problemas como valores atípicos, datos
        faltantes y multicolinealidad.
    -   Escalar o transformar las variables si es necesario,
        especialmente si están en diferentes escalas o presentan
        distribuciones no lineales.
4.  **Ajuste del modelo:**
    -   Especificar el modelo de regresión múltiple en su forma
        general:\
        $$
        Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p + \varepsilon,
         $$ donde $\varepsilon$ representa los errores aleatorios.
    -   Estimar los coeficientes del modelo
        ($\beta_0, \beta_1, \dots, \beta_p$) utilizando el método de
        mínimos cuadrados, que minimiza la suma de los errores al
        cuadrado.
5.  **Evaluación del modelo:**
    -   Analizar el ajuste general del modelo utilizando métricas como
        $R^2$ y $R^2$ ajustado, que miden la proporción de la
        variabilidad explicada.
    -   Examinar la tabla ANOVA para evaluar la significancia global del
        modelo.
    -   Realizar pruebas de hipótesis para los coeficientes
        individuales, verificando si las variables predictoras tienen un
        efecto significativo en la variable respuesta.
6.  **Diagnóstico del modelo:**
    -   Examinar los residuos para evaluar supuestos como la linealidad,
        homocedasticidad, normalidad de los errores y ausencia de
        autocorrelación.
    -   Identificar observaciones atípicas, leverage y puntos de
        influencia utilizando herramientas como la distancia de Cook,
        DFBETAS y DFFITS.
7.  **Reducción de variables:**
    -   En análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.
8.  **Validación del modelo:**
    -   Evaluar el desempeño del modelo con datos de validación o
        mediante técnicas como validación cruzada para garantizar su
        capacidad predictiva en nuevos conjuntos de datos.

El objetivo principal de este tema es presentar las técnicas más
relevantes para la selección de variables y regularización, entender sus
fundamentos teóricos, y aplicarlas a casos prácticos. Esto no solo
permitirá construir modelos más robustos y eficientes, sino que también
ayudará a obtener insights más claros y útiles a partir de los datos.

## Reducción de variables

En análisis de regresión, especialmente cuando se trabaja con conjuntos de datos de alta dimensionalidad, es común enfrentar situaciones en las que el número de variables explicativas es muy grande. Esto puede llevar a problemas como el sobreajuste, dificultades en la interpretación del modelo y una mayor complejidad computacional. Por ello, reducir el número de variables explicativas, sin perder información relevante, se convierte en un paso crucial para construir modelos más eficientes y robustos.

Es especialmente importante reducir el número de variables explicativas en los estudios observacionales exploratorios, ya que en otros tipos de estudios, como los diseñados previamente, las variables incluidas suelen estar seleccionadas de antemano porque se conoce su relación con la variable respuesta o porque han sido identificadas como relevantes en investigaciones previas.

La **reducción de variables explicativas** busca simplificar el modelo al seleccionar un subconjunto de predictores que capturen la mayor parte de la información relevante de los datos. Este proceso puede realizarse a través de diferentes enfoques, dependiendo del contexto y de las características del conjunto de datos.

### Motivaciones para reducir variables

Al limitar el número de predictores, no solo se simplifica el modelo, sino que también se optimizan diversos aspectos fundamentales en el análisis.

1. **Evitar el zobreajuste:**
   - Cuando hay demasiadas variables en relación al número de observaciones, el modelo puede ajustarse demasiado a los datos de entrenamiento y perder capacidad predictiva en nuevos conjuntos de datos.
2. **Mejorar la interpretabilidad:**
   - Un modelo con menos variables es más fácil de interpretar, lo que resulta fundamental en aplicaciones como ciencias sociales, biomedicina o economía.
3. **Reducción de complejidad computacional:**
   - Al disminuir el número de variables, se reducen los costos de tiempo y memoria en el ajuste y evaluación del modelo.
4. **Manejo de multicolinealidad:**
   - La reducción puede eliminar variables redundantes que presentan una alta correlación entre sí, estabilizando las estimaciones del modelo.

### Métodos de reducción de variables

Algunas de las ideas más comunes para tratar de reducir el número de variables de un modelo son: 

1. **Selección de variables:**
   - Utiliza estrategias como selección directa, métodos automáticos (forward, backward o stepwise), o técnicas basadas en regularización (Lasso, Elastic Net) para seleccionar las variables más relevantes.
   
2. **Técnicas de transformación de datos:**
   - Se proyectan las variables explicativas en un nuevo espacio de menor dimensionalidad, manteniendo la mayor cantidad posible de información. Algunas de estas técnicas se estudian en la asignatura de Aprendizaje Automático:
     - **Análisis de Componentes Principales (PCA):** Reduce las variables explicativas a un conjunto de componentes ortogonales que explican la mayor parte de la varianza.
     - **Análisis de Factores:** Agrupa variables relacionadas en factores latentes que capturan la esencia de la información.

3. **Filtrado basado en información:**
   - Identifica y descarta variables con baja variabilidad o poca relación con la variable respuesta, utilizando métricas como la correlación o importancia estadística.

4. **Métodos de selección basados en modelos:**
   - Ajusta modelos iterativamente para evaluar la contribución de cada variable explicativa y descartar aquellas con menor relevancia según criterios como el $p$-valor, AIC o BIC.

::: {.callout-important title="Consideraciones importantes"}

- La reducción de variables debe realizarse cuidadosamente para evitar la pérdida de información clave que pueda comprometer la calidad del modelo.
- Es fundamental validar el modelo resultante, asegurándose de que mantenga su capacidad predictiva mediante técnicas como validación cruzada.
- En algunos casos, la selección o transformación de variables puede implicar compromisos entre simplicidad e interpretabilidad.
:::

## Selección de variables

Simplificar un modelo eliminando variables irrelevantes es fundamental para mejorar su parsimonia y evitar el sobreajuste. Este objetivo puede lograrse mediante métodos de selección de variables, ya sean directos, automáticos o basados en regularización. Estas técnicas permiten identificar subconjuntos óptimos de predictores, optimizando tanto la simplicidad como la precisión del modelo. En particular, los métodos de regularización, como Ridge, Lasso y Elastic Net, introducen penalizaciones al modelo para controlar la complejidad y prevenir el sobreajuste, convirtiéndose en herramientas clave en el análisis de datos modernos.

Cuando se dispone de $p$ variables explicativas, es posible construir hasta $2^p$ modelos diferentes considerando todas las combinaciones posibles de estas variables. Sin embargo, explorar de manera exhaustiva todos estos modelos puede ser inviable, especialmente si $p$ es grande. Por ejemplo, con solo 10 variables regresoras, se generarían $2^{10} = 1024$ modelos posibles. Aunque la tecnología actual permite ajustar todos estos modelos, evaluar cada uno en términos de bondad de ajuste, gráficos de residuos, detección de observaciones influyentes y otros diagnósticos sería extremadamente complejo y costoso.

Para superar este desafío, se han desarrollado criterios específicos de selección de variables que ayudan a los analistas a identificar un pequeño subconjunto de modelos que cumplan con los estándares de calidad deseados. Este enfoque permite centrar el análisis en un grupo reducido de modelos “buenos”, generalmente entre 4 y 6, y realizar un estudio más profundo y detallado de ellos. Esta estrategia facilita tanto la interpretación como la eficiencia del proceso analítico, optimizando el uso de recursos computacionales y asegurando que los resultados sean robustos y fiables.


## Métodos de selección directa

Los métodos de selección directa son un enfoque fundamental en la
búsqueda de un subconjunto óptimo de variables predictoras en modelos de
regresión. Este enfoque evalúa de manera sistemática diferentes
combinaciones de variables para identificar cuál de ellas proporciona el
mejor ajuste al modelo en función de un criterio predefinido, como el
coeficiente de determinación ajustado ($R^2$ ajustado), el error
cuadrático medio (ECM) o criterios de información como AIC o BIC.

A diferencia de los métodos automáticos, los métodos de selección
directa no dependen de un proceso iterativo de adición o eliminación de
variables. En cambio, buscan exhaustivamente (o mediante aproximaciones
computacionalmente más eficientes) entre todas las posibles
combinaciones de variables, lo que garantiza un análisis completo de las
interacciones y relevancias potenciales.

Estos métodos son especialmente útiles cuando el número de predictores
no es demasiado grande, ya que el esfuerzo computacional crece
exponencialmente con el número de variables. Aunque el costo
computacional puede ser elevado en datasets amplios, los métodos de
selección directa proporcionan una referencia sólida y transparente para
evaluar qué variables son fundamentales en el modelo.

En esta sección, analizaremos los métodos de selección directa más
comunes, su implementación práctica y las métricas utilizadas para
comparar modelos, destacando sus ventajas y limitaciones.

## Métodos automáticos

Los métodos automáticos de selección de variables son herramientas
prácticas y eficientes diseñadas para identificar subconjuntos
relevantes de predictores en un modelo de regresión. A diferencia de los
métodos de selección directa, que exploran exhaustivamente todas las
combinaciones posibles de variables, los métodos automáticos siguen un
enfoque iterativo que simplifica el proceso de selección. Estos métodos
son especialmente útiles en situaciones donde el número de predictores
es elevado, ya que reducen significativamente el esfuerzo computacional.

El principio clave detrás de los métodos automáticos es el ajuste
dinámico del conjunto de variables en función de criterios estadísticos,
como $p$-valores, coeficientes de determinación ajustados ($R^2$)
ajustado), o criterios de información como AIC y BIC. Entre las
estrategias más comunes se encuentran:

-   **Método Forward (selección progresiva):** Parte de un modelo vacío
    e incorpora variables de manera secuencial, añadiendo en cada paso
    la variable que mejora más el modelo.

-   **Método Backward (eliminación regresiva):** Comienza con todas las
    variables en el modelo y elimina iterativamente aquellas que tienen
    menor impacto.

-   **Método Stepwise:** Combina las estrategias forward y backward,
    permitiendo tanto la inclusión como la exclusión de variables en
    cada iteración.

Estos métodos ofrecen una manera estructurada y ágil de seleccionar
variables, aunque no garantizan encontrar el mejor modelo global debido
a su naturaleza secuencial. A lo largo de esta sección, examinaremos
cada uno de estos métodos, sus ventajas, limitaciones y aplicaciones en
diferentes contextos de análisis.

## Métodos basados en regularización


En los modelos de regresión, especialmente cuando se trabaja con un gran
número de variables predictoras o con datos multicolineales, los métodos
tradicionales de selección de variables pueden resultar ineficaces o
inestables. En estos casos, los métodos basados en regularización surgen
como una alternativa poderosa que no solo selecciona variables, sino que
también mejora la estabilidad y la precisión del modelo.

La regularización consiste en introducir una penalización en la función
de ajuste del modelo, lo que tiene dos efectos principales: controlar el
sobreajuste al reducir la complejidad del modelo y forzar la selección
de un subconjunto más parsimonioso de predictores. Estas penalizaciones
ajustan los coeficientes de las variables predictoras, favoreciendo
soluciones más simples y robustas.

Entre los métodos de regularización más destacados se encuentran:

-   **Ridge Regression:** Aplica una penalización proporcional al
    cuadrado de los coeficientes, lo que permite manejar problemas de
    multicolinealidad pero no conduce a la eliminación completa de
    variables.
-   **Lasso (Least Absolute Shrinkage and Selection Operator):**
    Introduce una penalización basada en el valor absoluto de los
    coeficientes, lo que no solo reduce su magnitud, sino que también
    puede anularlos completamente, realizando una selección automática
    de variables.
-   **Elastic Net:** Combina las penalizaciones de Ridge y Lasso,
    ofreciendo mayor flexibilidad en situaciones donde hay una gran
    correlación entre los predictores.

Estos métodos son especialmente útiles en problemas donde el número de
variables predictoras excede el número de observaciones, o cuando se
desea un modelo más interpretable. En esta sección, exploraremos en
detalle los fundamentos teóricos, la implementación práctica y las
aplicaciones de cada uno de estos métodos, destacando sus ventajas en
escenarios complejos y desafiantes.
