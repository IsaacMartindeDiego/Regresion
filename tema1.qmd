---
editor: 
  markdown: 
    wrap: 72
---

# Modelo de regresión lineal: simple y múltiple {#sec-tema1}

La regresión lineal constituye uno de los pilares fundamentales de la
modelización estadística y es ampliamente utilizada en múltiples
disciplinas para analizar relaciones entre variables. Este modelo
permite explorar cómo una o varias variables explicativas
(independientes) influyen sobre una variable respuesta (dependiente),
proporcionando no solo descripciones útiles sino también herramientas
para la predicción y la inferencia.

El **modelo de regresión lineal simple** se centra en analizar la
relación entre una única variable explicativa y una respuesta, mientras
que el **modelo de regresión lineal múltiple** extiende este concepto al
considerar múltiples variables explicativas, permitiendo capturar
relaciones más complejas y realistas. Ambos modelos comparten principios
fundamentales, como el ajuste de una recta mediante el criterio de
mínimos cuadrados y la interpretación de sus parámetros, pero difieren
en su alcance y en los retos que presentan.

En este capítulo, abordaremos los fundamentos de la regresión lineal
simple, incluyendo sus suposiciones clave, el procedimiento de
estimación de parámetros y el uso de herramientas de diagnóstico. A
través de ejemplos prácticos, como los estudios clásicos de Galton sobre
la herencia de estaturas o el análisis de datos de inversión en
publicidad, se ilustrará el poder descriptivo y predictivo de este
modelo.

Este tema también prepara las bases para comprender el modelo de
regresión lineal múltiple, presentado como unageneralización del
simple, y para abordar las complejidades adicionales que surgen, como la
colinealidad entre variables explicativas y la selección de modelos
óptimos.

La comprensión y aplicación correcta de estos modelos es esencial no
solo para su utilidad en contextos prácticos, sino también porque
representan el punto de partida para técnicas más avanzadas en la
ciencia de datos y el aprendizaje automático.

::: {.callout-important title="Objetivos"}
Los siguiente objetivos buscan sentar las bases teóricas y prácticas
necesarias para entender y aplicar modelos de regresión, preparando al
estudiante para explorar modelos más complejos en los temas posteriores.

1.  **Introducción a la Modelización estadística**:
    -   Comprender el proceso de modelización estadística, desde la
        definición de objetivos y variables hasta el ajuste y evaluación
        del modelo.
2.  **Relación lineal y correlación**:
    -   Introducir conceptos como la correlación y la relación lineal
        entre variables.\
    -   Analizar cómo identificar y medir la fuerza de la relación entre
        dos variables.
3.  **Comprensión del modelo de regresión lineal simple**:
    -   Describir la estructura del modelo de regresión lineal simple y
        entender su formulación matemática.\
    -   Interpretar los parámetros del modelo (intercepto y pendiente) y
        su significado en un contexto práctico.
4.  **Procedimiento de inferencia estadística**:
    -   Estimar los coeficientes del modelo mediante el método de
        mínimos cuadrados.\
    -   Realizar inferencias sobre los parámetros, incluyendo contrastes
        de hipótesis y predicciones.
5.  **Diagnóstico del modelo**:
    -   Introducir herramientas para diagnosticar la adecuación del
        modelo ajustado.\
    -   Evaluar la validez de los supuestos del modelo, como la
        linealidad, homocedasticidad e independencia de errores.
6.  **Aplicación práctica**:
    -   Implementar el modelo en contextos reales, como el análisis de
        la influencia de inversiones en ganancias o la herencia de
        características biológicas (ejemplo de Galton).\
    -   Visualizar y analizar gráficamente las relaciones para facilitar
        la interpretación de los resultados.
:::

## Modelización estadística

La modelización estadística es un proceso estructurado que permite
analizar y describir relaciones entre variables mediante modelos
matemáticos. Este enfoque es fundamental en la estadística aplicada y
proporciona herramientas para interpretar datos, realizar predicciones y
tomar decisiones fundamentadas. A continuación, se describen los pasos
clave de este proceso. Este enfoque sistemático asegura que el modelo
ajustado sea robusto, interpretable y útil para realizar predicciones
confiables. La revisión continua y el ajuste basado en evidencia
permiten capturar la complejidad de los datos de manera efectiva.

### Contextualización del problema

El primer paso en la modelización estadística es definir el problema que
se busca analizar. Esto incluye identificar el contexto, establecer
objetivos claros y determinar las variables involucradas.

::: {.callout-tip title="Ejemplo" collapse="true"}
-   **Problema:** Investigar si existe una relación positiva entre el
    tiempo dedicado al estudio semanal y el promedio de calificaciones
    de los estudiantes universitarios.\
-   **Variables:**
    -   Variable explicativa (independiente): Tiempo de estudio semanal
        (en horas).\
    -   Variable respuesta (dependiente): Promedio de calificaciones al
        final del semestre (en una escala de 0 a 10).\
-   **Objetivo:** Determinar si los estudiantes que dedican más horas al
    estudio semanalmente obtienen mejores calificaciones en promedio.

Emplearemos datos simulados para este ejemplo:

```{r datos, warning=FALSE}
# Establecer la semilla para reproducibilidad
set.seed(123)

# Número de estudiantes
n <- 100

# Generar tiempo de estudio (en horas) como una variable independiente
tiempo_estudio <- round(runif(n, min = 5, max = 40), 1)

# Relación lineal entre tiempo de estudio y calificaciones (con ruido)
beta_0 <- 5  # Intercepto
beta_1 <- 0.1  # Pendiente (efecto del tiempo de estudio)
sigma <- 0.5  # Varianza del ruido

# Esta será la relación que deseamos "descubrir".
calificaciones <- round(beta_0 + beta_1 * tiempo_estudio + rnorm(n, mean = 0, sd = sigma), 2)

# Crear un data frame con los datos generados
datos <- data.frame(Tiempo_Estudio = tiempo_estudio, Calificaciones = calificaciones)

# Visualizar los primeros registros
head(datos)

```
:::

### Inspección gráfica e identificación de tendencias

Antes de ajustar un modelo, es esencial realizar una inspección visual
de los datos. Los gráficos de dispersión son una herramienta útil para
observar tendencias, relaciones o patrones entre las variables.

::: {.callout-tip title="Ejemplo" collapse="true"}
En el ejemplo, representamos el tiempo de estudio frente a las
calificaciones. De este modo podemos analizar si los puntos siguen un
patrón lineal o si muestran comportamientos más complejos.

```{r grafico1, warning=FALSE}

# Graficar los datos de estudio generados anteriormente
plot(datos$Tiempo_Estudio, datos$Calificaciones,
     main = "Relación entre Tiempo de Estudio y Calificaciones",
     xlab = "Tiempo de Estudio (horas/semana)",
     ylab = "Calificaciones (promedio)",
     pch = 19, col = "blue")

```
:::

### Propuesta y ajuste del modelo

Con base en las observaciones previas, se propone un modelo estadístico
que relacione las variables. En el caso de relaciones lineales, la
ecuación típica es:

$$
\text{Respuesta} = \beta_0 + \beta_1 (\text{Variable explicativa}) + \varepsilon,
$$

donde $\beta_0$ es la constante (o intercepto), $\beta_1$ es la
pendiente, y $\varepsilon$ es el término de error aleatorio.

Fíjate que la expresión anterior concuerda con el siguiente principio
fundamental del análisis de datos:

$$DATOS = MODELO + ERROR$$

-   Los **datos** representan la realidad (procesos de negocios,
    clientes, productos, actividades, fenómenos físicos, etc.) que se
    quiere comprender, predecir o mejorar.

-   El **modelo** es una representación **simplificada** de la realidad
    que proponemos para describirla e interpretarla más fácilmente.

-   El **error** refleja la diferencia entre nuestra representación
    simplificada de la realidad (el modelo) y los datos que relamente
    describen esa realidad de forma precisa.

::: {.callout-tip title="Ejemplo" collapse="true"}
-   Propuesta del modelo:\
    $$
    \text{Calificaciones} = \beta_0 + \beta_1 (\text{Tiempo de Estudio}).
    $$
-   **Ajuste:** Calcular los valores de \$ \beta\_0 \$ y \$ \beta\_1 \$
    mediante el método de mínimos cuadrados. Este método busca minimizar
    la suma de los errores cuadrados entre los valores observados y los
    predichos por el modelo.

```{r ajuste1, warning=FALSE}

# Ajustar un modelo de regresión lineal
modelo <- lm(Calificaciones ~ Tiempo_Estudio, data = datos)
summary(modelo)

# Graficar los datos de estudio y su recta de regresión lineal
plot(datos$Tiempo_Estudio, datos$Calificaciones,
     main = "Relación entre Tiempo de Estudio y Calificaciones",
     xlab = "Tiempo de Estudio (horas/semana)",
     ylab = "Calificaciones (promedio)",
     pch = 19, col = "blue")
abline(lm(Calificaciones ~ Tiempo_Estudio, data = datos), col = "red", lwd = 2)
```

A la vista de los estimadores de los parámetros del modelo, tenemos la
siguiente interpretación:

-   Se produce un incremento de la calificiación de $0.1$
    (aproximadamente) por cada incremento de $1$ hora en el tiempo de
    estudio. O bien, por cada $10$ horas de estudio, se incrementa la
    nota en $1$ punto.

-   En ocasiones es complicado interpretar el valor de $\beta_0$. En
    este caso, corresponde a la calificación de los estudiantes que no
    dedican ningún tiempo de estudio. En este caso sería $5$
    (aproximadamente).
:::

### Revisión y diagnóstico del modelo

Una vez ajustado el modelo, es crucial evaluar su calidad. Esto implica
analizar si los supuestos del modelo se cumplen, como la linealidad, la
homocedasticidad y la independencia de los errores.

Deberemos realizar las siguientes tareas:

-   Comparar los valores predichos por el modelo con los datos
    observados para verificar su ajuste.

-   Examinar los residuos (errores) para identificar posibles problemas,
    como tendencias no capturadas o varianzas no constantes.

### Reajuste del modelo

Si el diagnóstico revela deficiencias en el modelo ajustado, se plantean
modificaciones. Estas pueden incluir:

-   Transformaciones de variables para mejorar la linealidad.

-   Introducción de términos adicionales, como variables cuadráticas o
    interacciones.

-   Cambios a modelos no lineales si el comportamiento de los datos lo
    requiere.

::: {.callout-tip title="Estudios de Galton sobre estatura"}
Los estudios de Sir Francis Galton sobre la estatura son un ejemplo
clásico en estadística y forman parte de la historia de la regresión
lineal. Galton, un polímata británico del siglo XIX, investigó la
herencia biológica y publicó en 1889 su libro *Natural Inheritance*,
donde analizó datos sobre la relación entre las estaturas de padres e
hijos. Estos estudios no solo sentaron las bases para la regresión
lineal, sino que también ayudaron a formalizar conceptos clave en
estadística, haciendo de Galton una figura central en su desarrollo.

**Contexto y propósito**

Galton estaba interesado en cómo las características físicas, como la
estatura, se transmiten de padres a hijos. Su objetivo era cuantificar
esta relación y establecer patrones de herencia. En particular, buscó
responder si los hijos de padres altos tienden a ser más altos y si los
de padres bajos tienden a ser más bajos.

**Datos recopilados**

-   Galton recopiló datos sobre las estaturas de **928 hijos** y sus
    respectivos **padres**.
-   Las medidas fueron expresadas en pulgadas (1 pulgada = 2.54 cm).\
-   En sus análisis, utilizó el promedio de las estaturas de ambos
    padres, conocido como **estatura media parental**, para compararlo
    con la estatura de los hijos.

**Principales hallazgos**

1.  **Relación lineal entre padres e hijos:**\
    Galton observó que existe una relación positiva entre la estatura de
    los padres y la de los hijos. Los padres altos tienden a tener hijos
    altos, y los padres bajos tienden a tener hijos bajos. Esta relación
    puede modelarse con una línea recta, lo que inspiró la formulación
    de la regresión lineal.

2.  **Regresión a la media:**

    -   Aunque los hijos de padres altos son, en promedio, más altos que
        el promedio general de la población, también tienden a ser
        **menos altos que sus padres**.\
    -   De manera similar, los hijos de padres bajos son más bajos que
        el promedio general, pero suelen ser **menos bajos que sus
        padres**.\
    -   Este fenómeno, que Galton llamó "regresión a la media", ocurre
        porque las características extremas tienden a suavizarse en la
        siguiente generación debido a la influencia de múltiples
        factores genéticos y ambientales.

3.  **Ecuación de la recta de regresión:**\
    Galton ajustó una recta para describir la relación entre la estatura
    media parental ($X$) y la estatura de los hijos ($Y$): $$
    Y = \beta_0 + \beta_1 X
    $$ Donde:

    -   $\beta_0$: Intercepto, representa la estatura promedio de los
        hijos cuando la estatura parental es promedio.
    -   $\beta_1$: Pendiente, indica cómo cambia la estatura de los
        hijos por cada unidad de cambio en la estatura media parental.

**Importancia en la Estadística**

1.  **Regresión lineal:**\
    Este estudio introdujo el concepto de **recta de regresión**, que
    describe cómo varía la media de una variable dependiente en función
    de una variable independiente.

2.  **Correlación:**\
    Galton también estudió el grado de relación entre variables,
    precursor del concepto de **coeficiente de correlación**
    desarrollado posteriormente por Karl Pearson, un discípulo suyo.

3.  **Regresión a la media:**\
    El término y la idea detrás de "regresión a la media" surgieron de
    estos estudios y son hoy fundamentales en estadística y genética.

**Ejemplo Gráfico**

Galton representó sus datos en gráficos de dispersión, mostrando cómo
los puntos (pares de estatura media parental y estatura de los hijos) se
agrupan alrededor de la recta de regresión, ilustrando la tendencia
general de la relación.

```{r galtonn}
# Instalar y cargar los paquetes necesarios
library(ggplot2)
library(HistData)

# Cargar los datos de Galton
data("GaltonFamilies")
galton_data <- GaltonFamilies

# Crear el modelo de regresión lineal
modelo <- lm(childHeight ~ midparentHeight, data = galton_data)

# Crear el gráfico con ggplot2
grafico <- ggplot(galton_data, aes(x = midparentHeight, y = childHeight)) +
  geom_point() +  # Añadir puntos de datos
  geom_smooth(method = "lm", col = "red") +  # Añadir la recta de regresión
  labs(title = "Altura de Padres e Hijos (Datos de Galton)",
       x = "Altura de los Padres",
       y = "Altura de los Hijos") +
  annotate("text", x = 67, y = 75, label = paste("y =", round(coef(modelo)[1], 2), "+", round(coef(modelo)[2], 2), "x"), color = "red")

# Mostrar el gráfico
print(grafico)
```
:::

## Correlación

La correlación es una medida estadística que describe la relación entre
dos variables. Permite evaluar si existe una asociación entre ellas y en
qué grado los cambios en una variable están relacionados con los cambios
en la otra. En el contexto de modelos de regresión, la correlación es un
paso fundamental para explorar la fuerza y la dirección de la relación
entre las variables explicativas y la variable respuesta.

::: {.callout-note collapse="true"}
## Concepto de correlación

La correlación responde a preguntas como:

-   ¿A valores altos de una variable le corresponden valores altos de la
    otra? (correlación positiva).
-   ¿A valores altos de una variable le corresponden valores bajos de la
    otra? (correlación negativa).
-   ¿No existe un patrón evidente de asociación? (ausencia de
    correlación).
:::

### Covarianza

La **covarianza** es una medida que cuantifica cómo varían conjuntamente
dos variables. Su fórmula es:\
$$
\text{Cov}(X, Y) = \frac{\sum_{i=1}^{n}(X_i - \bar{X})(Y_i - \bar{Y})}{n}
$$

-   Si la covarianza es positiva, indica una tendencia a que ambas
    variables aumenten juntas (relación positiva).

-   Si es negativa, una variable tiende a aumentar mientras la otra
    disminuye (relación negativa).\

-   Una covarianza cercana a cero sugiere que no hay una relación lineal
    significativa.

Sin embargo, la covarianza tiene una limitación: su valor depende de las
unidades de medida de las variables, lo que dificulta su interpretación
directa.

::: {.callout-tip title="Ejemplo" collapse="true"}
```{r galton_covarianza}
# Cargar los datos de Galton
data("GaltonFamilies")
galton_data <- GaltonFamilies

# Seleccionar las variables de interés
midparent_height <- galton_data$midparentHeight
child_height <- galton_data$childHeight

# Calcular la covarianza
covarianza <- round(cov(midparent_height, child_height),3)

# Mostrar el resultado
print(paste("La covarianza entre la altura de los padres y la altura de los hijos es:", covarianza))
```
:::

### Coeficiente de correlación lineal

Para superar la limitación anterior, se utiliza el **coeficiente de
correlación lineal de Pearson** ($r$), que estandariza la covarianza
dividiéndola por las desviaciones típicas de las variables:

$$
r(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \cdot \sigma_Y}
$$

**Valores de** $r$:

-   $r = 1$: Correlación perfectamente positiva (todos los puntos caen
    en una línea recta creciente).\
-   $r = -1$: Correlación perfectamente negativa (todos los puntos caen
    en una línea recta decreciente).\
-   $r = 0$: Ausencia de correlación lineal.

El coeficiente de correlación es adimensional y toma valores entre $-1$
y $1$, facilitando su interpretación sin importar las unidades de las
variables.

::: {.callout-tip title="Ejemplo" collapse="true"}
```{r galton_corr}
# Cargar los datos de Galton
data("GaltonFamilies")
galton_data <- GaltonFamilies

# Seleccionar las variables de interés
midparent_height <- galton_data$midparentHeight
child_height <- galton_data$childHeight

# Calcular la covarianza
correlacion_pearson <- round(cor(midparent_height, child_height),2)

# Mostrar el resultado
print(paste("El coeficiente de correlación de Pearson entre la altura de los padres y la altura de los hijos es:", correlacion_pearson))
```
:::

**Interpretación del Coeficiente de Correlación**

1.  **Magnitud:**
    -   Valores cercanos a $|1|$ indican una relación lineal fuerte.\
    -   Valores cercanos a $0$ sugieren una relación lineal débil o
        inexistente.
2.  **Signo:**
    -   **Positivo:** Ambas variables tienden a moverse en la misma
        dirección.\
    -   **Negativo:** Las variables tienden a moverse en direcciones
        opuestas.

**Relación lineal y otras relaciones**

Es importante destacar que una correlación de $r = 0$ no implica
necesariamente que no haya relación entre las variables. Puede haber una
relación no lineal que el coeficiente de correlación lineal no detecta.

::: {.callout-tip title="Ejemplo" collapse="false"}
Por ejemplo, en un gráfico de dispersión en forma de parábola, el
coeficiente de correlación lineal podría ser cercano a 0, a pesar de que
existe una relación cuadrática clara entre las variables.

```{r corr_0}
# Generar datos de ejemplo
set.seed(0)
x <- seq(-10, 10, length.out = 100)
y <- x^2 + rnorm(100, mean = 0, sd = 10)

# Calcular el coeficiente de correlación de Pearson
correlacion_pearson <- cor(x, y)

# Crear el gráfico de dispersión
plot(x, y, main = "Gráfico de Dispersión con Relación Cuadrática",
     xlab = "X", ylab = "Y", pch = 19)
abline(h = 0, v = 0, col = "gray", lty = 2)

# Mostrar el coeficiente de correlación en el gráfico
text(0, 80, paste("Correlación de Pearson:", round(correlacion_pearson, 2)), col = "red")
```
:::

### Aplicación práctica

La correlación es una herramienta inicial esencial en la exploración de
datos:

-   Identifica variables explicativas potenciales para modelos de
    regresión.\
-   Ayuda a entender la estructura de los datos y a verificar si una
    relación lineal es razonable.

::: {.callout-tip title="Ejemplo" collapse="false"}
En un análisis del tiempo de estudio semanal y las calificaciones, se
puede calcular el coeficiente de correlación $r$ para determinar si los
estudiantes que estudian más tienden a obtener mejores resultados.

```{r corr_practico_1}
# Generar datos de ejemplo
# Calcular el coeficiente de correlación de Pearson
coef_correlacion <- round(cor(datos$Tiempo_Estudio, datos$Calificaciones),3)

# Mostrar el coeficiente de correlación
cat("El coeficiente de correlación entre el tiempo de estudio y las calificaciones es:", coef_correlacion, "\n")

```
:::

## Regresión lineal simple

La regresión lineal simple es una de las herramientas más fundamentales
y ampliamente utilizadas en el análisis estadístico. Su objetivo
principal es modelar la relación entre dos variables: una variable
explicativa (independiente) y una variable respuesta (dependiente). Este
modelo permite no solo describir cómo se relacionan estas dos variables,
sino también realizar predicciones basadas en dicha relación.

El concepto básico de la regresión lineal simple es ajustar una recta
que minimice las discrepancias entre los valores observados y los
predichos por el modelo. La ecuación general de este modelo es:\
$$
Y = \beta_0 + \beta_1 X + \varepsilon,
$$ donde:

  -   $Y$ es la variable dependiente o respuesta.\
  -   $X$ es la variable independiente o explicativa.\
  -   La ecuación recibe el nombre de **recta de regresión**.\
  -   Los coeficiente $\beta_0$ y $\beta_1$ reciben el nombre de **coeficientes del modelo de regresión**.
      -   $\beta_0$ es el intercepto, que representa el valor de $Y$ cuando
    $X = 0$.\
      -  $\beta_1$ es la pendiente, que indica el cambio esperado en $Y$ por
    cada unidad de cambio en $X$.\
  -   $\varepsilon$ es un término de error que captura la variabilidad no
    explicada por el modelo.

Para cada valor fijo de $X$, $Y$ es una variable aleatoria, es decir, $Y$ sigue una distribución de probabilidad para cada valor fijo de $X$, con media o valor esperado:
$$
E[Y|X]=\beta_0+\beta_1X,
$$
y con varianza:
$$
Var(Y|X)=Var(\beta_0+\beta_1X+\varepsilon)=Var(\varepsilon)=\sigma^2.
$$

::: {.callout-note title="Propiedades Clave" collapse="false"}
Para que las inferencias del modelo sean válidas, se requieren ciertos
supuestos sobre los datos y el término de error $\varepsilon$:

1.  **Linealidad:** La relación entre $X$ y $Y$ es lineal. Es decir, la esperanza de $Y$ es una función **lineal** de $X$.\
2.  **Independencia:** Los términos de error $\varepsilon$ son
    independientes entre sí. Los errores para observaciones distintas son incorrelados, es decir $\varepsilon_i$ y $\varepsilon_j$ son no correlados y, por tanto, también lo son $Y_i$ e $Y_j$\.
3.  **Homocedasticidad:** La varianza de los términos de error
    $\varepsilon$ es constante para todos los valores de $X$. Es decir, la varianza de $Y$ es constante, no depende $X$.\

Cuando el objetivo no es sólo estimar la recta, sino inferir con ella, entonces se asume una hipótesis más: la normalidad de la variable respuesta, o lo que es lo mismo, del error aleatorio:
4.  **Normalidad:** Los términos de error $\varepsilon$ siguen una
    distribución normal con media cero y varianza constante:\
    $$
    \varepsilon_i \overset{\mathrm{iid}}{\sim} N(0, \sigma^2) , \hspace{0.5cm} i=1,\ldots,n
    $$


Estos supuestos son esenciales para garantizar la validez de las
estimaciones y conclusiones derivadas del modelo.
:::

En resumen, el modelo de regresión lineal simple, implica que las respuestas $Y_i$ proceden de una distribución de probabilidad con esperanza:
$$
E[Y_i|X_i]=\beta_0+\beta_1X_i \hspace{.5cm} i=1,\dots,n,
$$
y con varianza $\sigma^2$, para todas las $Y$. Además, cualesquiera dos respuestas $Y_i$ e $Y_j$ son incorreladas.

La regresión lineal simple es fundamental en estadística y ciencia de
datos porque proporciona un marco intuitivo y matemáticamente riguroso
para analizar relaciones. Algunas aplicaciones comunes incluyen:

-   Predecir valores futuros, como ingresos o gastos, en función de un
    predictor.\
-   Evaluar el impacto de una variable en otra, como el efecto de la
    inversión publicitaria en las ventas.\
-   Explorar relaciones lineales entre variables en estudios científicos
    o sociales.
    
En esta sección, exploraremos los fundamentos de la regresión lineal
simple, desde su formulación teórica hasta su implementación práctica.
Veremos cómo ajustar este modelo, interpretar sus parámetros, y evaluar
su adecuación utilizando herramientas estadísticas y gráficas. Además,
se presentarán ejemplos prácticos para ilustrar su aplicación en
situaciones reales.

### Estimación de los parámetros del modelo

Los parámetros del modelo ($\beta_0$ y $\beta_1$) se estiman utilizando
el **método de mínimos cuadrados**. Llamaremos $\hat{\beta_0}$ y
$\hat{\beta_1}$ a los estimadores de los parámetros.

Llamaremos **valor predicho (**$\hat{Y}$):\
$$
  \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i,
  $$ al valor estimado de $Y$ para un valor dado de $X$.

Y nos referimos al **residuo (**$e_i$):\
$$
  e_i = Y_i - \hat{Y}_i,
  $$ para medir la discrepancia entre el valor observado $Y_i$ y el
valor predicho $\hat{Y}_i$.

El método de los mínimos cuadrados busca minimizar la suma de los
errores cuadráticos (residuos) entre los valores observados de $Y$ y los
valores predichos ($\hat{Y}$):

$$
\text{SSE} = \sum_{i=1}^n e_i^2 =\sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} \left(Y_i - (\hat{\beta_0} + \hat{\beta_1} X_i)\right)^2.
$$

::: {.callout-note title="Minimización de SSE" collapse="true"}
La obtención de los estimadores de mínimos cuadrados para la regresión
lineal simple se basa en minimizar la suma de los cuadrados de los
residuos ($SSE$). Aquí está el proceso paso a paso:

Para minimizar $SSE$, derivamos parcialmente con respecto a $\beta_0$ y
$\beta_1$ y resolvemos el sistema de ecuaciones.

1.  **Primera derivada con respecto a** $\beta_0$: $$
    \frac{\partial SSE}{\partial \beta_0} = -2 \sum_{i=1}^n \left(Y_i - (\beta_0 + \beta_1 X_i)\right).
    $$ Igualando a cero: $$
    \sum_{i=1}^n \left(Y_i - \beta_0 - \beta_1 X_i\right) = 0.
    $$

    Reordenando: $$
    n\beta_0 + \beta_1 \sum_{i=1}^n X_i = \sum_{i=1}^n Y_i. \tag{1}
    $$

2.  **Primera derivada con respecto a** $\beta_1$: $$
    \frac{\partial SSE}{\partial \beta_1} = -2 \sum_{i=1}^n X_i \left(Y_i - (\beta_0 + \beta_1 X_i)\right).
    $$ Igualando a cero: $$
    \sum_{i=1}^n X_i \left(Y_i - \beta_0 - \beta_1 X_i\right) = 0.
    $$

    Reordenando: $$
    \beta_0 \sum_{i=1}^n X_i + \beta_1 \sum_{i=1}^n X_i^2 = \sum_{i=1}^n X_i Y_i. \tag{2}
    $$

**Resolución del Sistema de Ecuaciones**

El sistema está dado por las ecuaciones (1) y (2):

1.  $n\beta_0 + \beta_1 \sum_{i=1}^n X_i = \sum_{i=1}^n Y_i.$\
2.  $\beta_0 \sum_{i=1}^n X_i + \beta_1 \sum_{i=1}^n X_i^2 = \sum_{i=1}^n X_i Y_i.$

Resolviendo para $\beta_0$ y $\beta_1$:

1.  De la primera ecuación, despejamos $\beta_0$:\
    $$
    \beta_0 = \frac{\sum_{i=1}^n Y_i - \beta_1 \sum_{i=1}^n X_i}{n}. \tag{3}
    $$

2.  Sustituimos $\beta_0$ en la segunda ecuación:\
    $$
    \frac{\sum_{i=1}^n Y_i - \beta_1 \sum_{i=1}^n X_i}{n} \sum_{i=1}^n X_i + \beta_1 \sum_{i=1}^n X_i^2 = \sum_{i=1}^n X_i Y_i.
    $$

    Simplificando:\
    $$
    \beta_1 \left(\sum_{i=1}^n X_i^2 - \frac{(\sum_{i=1}^n X_i)^2}{n}\right) = \sum_{i=1}^n X_i Y_i - \frac{\sum_{i=1}^n X_i \sum_{i=1}^n Y_i}{n}.
    $$

3.  Expresamos $\beta_1$:\
    $$
    \beta_1 = \frac{\sum_{i=1}^n X_i Y_i - \frac{\sum_{i=1}^n X_i \sum_{i=1}^n Y_i}{n}}{\sum_{i=1}^n X_i^2 - \frac{(\sum_{i=1}^n X_i)^2}{n}}.
    $$ Esta es la fórmula para $\beta_1$, que puede reescribirse como:\
    $$
    \beta_1 = \frac{\text{Cov}(X, Y)}{\text{Var}(X)},
    $$ donde $\text{Cov}(X, Y)$ y $\text{Var}(X)$ son la covarianza y la
    varianza muestral de $X$ y $Y$.

4.  Finalmente, sustituimos $\beta_1$ en la ecuación (3) para obtener
    $\beta_0$:\
    $$
    \beta_0 = \bar{Y} - \beta_1 \bar{X},
    $$ donde $\bar{X}$ y $\bar{Y}$ son las medias de $X$ y $Y$.
:::

Las fórmulas para los estimadores de mínimos cuadrados son:\
$$
\hat{\beta}_1 = \frac{\text{Cov}(X, Y)}{\text{Var}(X)} = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2},
$$ $$
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X},
$$ donde $\bar{X}$ y $\bar{Y}$ son las medias de $X$ y $Y$,
respectivamente.


::: {.callout-caution title="Ejercicio" collapse="true"}
Asumiendo la normalidad de la variable respuesta:
$$
    \varepsilon_i \overset{\mathrm{iid}}{\sim} N(0, \sigma^2) \Leftrightarrow Y_i \overset{\mathrm{iid}}{\sim} N(\beta_0+\beta_1X_i,\sigma^2), \hspace{0.5cm} i=1,\ldots,n.
$$
Se tiene entonces la verosimilud para $\beta=(\beta_0,\beta_1)$,
$$
L(\beta; Y)=exp \left ( -\frac{\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2}{2\sigma^2} \right).
$$

En este ejercicio, pedimos buscar los valores de $\beta$ tales que maximizan la verosimilud. Esto es, los estimadores máximo verosimiles. Para ello derivamos e igualamos a cero. Se pide demostrar que los estimadores máximo verosimiles coinciden con los obtenidos por mínimos cuadrados.
:::

### Propiedades de los estimadores

Estudiemos algunas de las propiedades de los estimadores de mı́nimos
cuadrados y del modelo de regresión ajustado.

-   Sean: $$
    S_{xx}=\sum_{i=1}^n (X_i-\bar{X})^2
    $$ y $$
    S_{xy}=\sum_{i=1}^n (X_i-\bar{X})(Y_i-\bar{Y})
    $$ Entonces: $$
    \hat{\beta_1}=\frac{S_{xy}}{S_{xx}}
    $$

-   Los estimadores de $\beta_0$ y $\beta_1$ son insesgados, es decir:
    $$
    E[\hat{\beta_0}]=\beta_0, \space E[\hat{\beta_1}]=\beta_1
    $$

-   Las varianzas de los estimadores de $\beta_0$ y $\beta_1$ son: $$
    Var(\hat{\beta_1})=\frac{\sigma^2}{S_{xx}}
    $$ donde $\sigma^2$ es la varianza del error $\varepsilon$. En la
    práctica, como $\sigma^2$ no se conoce, se estima con la varianza
    residual ($\hat{\sigma}^2$): $$
    \hat{\sigma}^2=\frac{SSE}{n-2},
    $$ siendo $SSE=\sum_{i=1}^{n}(Y_i-\hat{Y_i})^2$, la suma de los
    cuadrados de los residos y $n-2$ los grados de libertad (por estimar
    dos parámetros). De modo que:

$$
Var(\hat{\beta_1}) \approx \frac{\hat{\sigma}^2}{S_{xx}}
$$

De igual modo tenemos: 
$$
Var(\hat{\beta_0}) \approx \hat{\sigma}^2 \left ( \frac{1}{n}+\frac{\bar{X}^2}{S_{xx}}\right )
$$

  - Teorema de Gauss-Markov: Para el modelo de regresión con $E[\varepsilon] = 0$,
$Var(\varepsilon) = \sigma^2$ y los errores incorrelados, se tiene que los estimadores $\hat{\beta_0}$ y  $\hat{\beta_1}$ son insesgados
y de mı́nima varianza. Demostración en [@kutner2005applied].

::: {.callout-note title="Propiedades adicionales para las predicciones y para los residuos" collapse="false"}

  - La suma de los residuos es cero:
  $$
  \sum_{i=1}^n e_i=\sum_{i=1}^n(Y_i-\hat{Y_i})=0
  $$
  

  - La suma de los valores observados es igual a la suma de los valores ajustados:
  $$
  \sum_{i=1}^n Y_i=\sum_{i=1}^n \hat{Y_i}
  $$
  
  - La suma de los residuos ponderados por los regresores es cero:
  $$
  \sum_{i=1}^n X_ie_i=0
  $$

  - La suma de los residuos ponderados por las predicciones es cero:
  $$
  \sum_{i=1}^n \hat{Y_i}e_i=0
  $$
  
  - La recta de regresión contiene el punto $(\bar{X},\bar{Y})$:
  
:::


::: {.callout-tip title="Ejemplo" collapse="true"}
Para los datos de calificaciones y tiempo de estudio, estos son los estimadores de los parámetros del modelo de regresión:

```{r estimadores}
# Ajustar el modelo de regresión lineal
modelo <- lm(calificaciones ~ tiempo_estudio)

# Obtener estimadores de los parámetros y errores estándar
estimadores <- coef(summary(modelo))  # Incluye estimadores y errores estándar

# Imprimir los resultados
cat("Estimadores para los parámetros del modelo:\n")
print(estimadores)
```
:::


Una vez obtenida la recta de regresión, surgen una serie de preguntas interesantes:

  - ¿Cómo de bien describe la ecuación de regresión los datos observados?
  - ¿Podemos emplear este modelo para predecir nuevas observaciones?
  - ¿Se cumplen todas las suposiciones del modelo?
  
Todas estas cuestiones serán analizadas antes de adoptar el modelo como válido. Una herramienta muy importante en la validación del modelo será el estudio de los residuos. Lo veremos más adelante.



### Inferencia sobre los parámetros del modelo

En regresión, con frecuencia interesa realizar contraste de hipótesis o
construir intervalos de confianza para los parámetros del
modelo. 

Para hacer inferencias consideramos la distribución en el
muestreo de los estimadores de dichos parámetros. Este procedimiento requiere que
tengamos en cuenta la suposición de normalidad sobre los errores
$\varepsilon_i$ , es decir:
$$
    \varepsilon_i \overset{\mathrm{iid}}{\sim} N(0, \sigma^2) 
$$
Entonces:
$$
Y_i\sim N(\beta_0+\beta_1X,\sigma^2),
$$
y
se puede demostrar que:
$$
\hat{\beta_1} \sim N\left (\beta_1,\frac{\sigma^2}{S_{xx}}\right),
$$
$$
\hat{\beta_0} \sim N\left ( \beta_0, \sigma^2 \left ( \frac{1}{n} + \frac{\bar{X}^2}{S_{xx}} \right) \right )
$$

Puesto que las varianzas de los parámetros1 dependen de $\sigma^2$ , cuando
el modelo de regresión es adecuado, podemos estimarlas sustituyendo dicho valor por su estimador insesgado $s^2$. En este caso, la distribución de los parámetros ya no será normal, al igual que ocurrı́a en la distribución
del valor esperado $\mu$ en una población i.i.d., aparece la distribución t-Student:
$$
t=\frac{\hat{\beta_1}-\beta_1}{SE({\hat{\beta_1}})}\sim t_{n-2},
$$
siendo $SE(\hat{\beta_1})=\frac{s}{\sqrt{S_{xx}}}$, es una t-Student con $n-2$ grados de ligertad. 

#### Intervalo de confianza

Así, su intervalo de confianza al $1-\alpha\%$ calculado a partir de la distribución en el muestro es:
$$
IC(\beta_1;1-\alpha)=\hat{\beta_1} \pm t_{1-\alpha/2,n-2}\sqrt{\frac{s^2}{S_{xx}}},
$$
donde $t_{1-\alpha/2,n-2}$ es el cuantil $1-\alpha/2$ de una distribución $t$ con $n-2$ grados de libertad (los correspondientes a $s^2$).


#### Contraste de hipótesis

En regresión, normalmente estamos interesado en realizar el siguiente contraste de hipótesis:
$$
H_0:\beta_1=0 \text{ frente a } H_1: \beta_1\neq 0.
$$
De este modo, si no podemos rechazar la hipótesis nula, podemos concluir que no existe prueba (en los datos) de una relación lineal significativa entre las variables estudiadas. Este tipo de contraste nos será muy útil cuando trabajemos con modelos con más de una variable para eliminar aquellas que no son importantes dentro del mismo.

En el caso del modelo de regresión lineal simple, no rechazar $H_0$ implica que la mejor predicción para todas las observaciones es: $\hat{Y_i}=Y_i$, o bien que la relación entre las variables $X$ e $Y$ no es lineal y, entonces, demos corregir nuestro modelo. 

Por otro lado, rechazar la hipótesis nula en favor de la alternativa, significa que la variable $X$ influye al explicar la variabilidad de la variable $Y$. Quizás el modelo no sea el más adecuado, pero no podemos eliminar la variable del mismo.

Para realizar el contraste, como $\varepsilon_i$ sin variables i.i.d. tales que  $\varepsilon_i\sim N(0,\sigma^2)$, entonces:

$$
Y_i\sim N(\beta_0+\beta_1X,\sigma^2), \text{ } \hat{\beta_1} \sim N\left (0,\frac{\sigma^2}{S_{xx}}\right)
$$

De manera que si, como es lo habitual, $\sigma^2$ es desconocida, como $E[\hat{\sigma^2}]=\sigma^2$, se tiene que, bajo la hipótesis nula:
$$
t=\frac{\hat{\beta_1}}{SE({\hat{\beta_1}})}\sim t_{n-2},
$$
donde $SE({\hat{\beta_1}})=\sqrt{\frac{\hat{\sigma^2}}{S_{xx}}}$, es el error estándar estimado del parámetro $\hat{\beta_1}$. De este modo, la hipótesis nula será rechazada si $|t|>t_{\alpha/2,n-2}$, siendo $\alpha$, el nivel de significación del contraste.

::: {.callout-important title="Para recordar"}
En los programas estadı́sticos se suele proporcionar el p-valor del contraste. Puedes repasar el significado de p-valor proporcionado en la asignatura de Inferencia.
:::



::: {.callout-tip title="Ejemplo" collapse="true"}
A continuación se presenta un resumen del modelo. Podemos ver los p-valores asociados a cada uno de los parámetros del modelo.

```{r estimadores2}
# Ajustar el modelo de regresión lineal
modelo <- lm(calificaciones ~ tiempo_estudio)

# Obtener resumen del modelo
summary(modelo)
```
:::

### Descomposición de la varianza: ANOVA

La descomposición de la varianza es un paso crucial para evaluar la calidad del ajuste del modelo de regresión lineal simple. A través de un análisis de varianza (ANOVA), se divide la variabilidad total de la variable respuesta ($Y$) en componentes atribuibles al modelo y al error, proporcionando un marco para evaluar si la relación entre $X$ e $Y$ es estadísticamente significativa. Un modelo es bueno si la variabilidad explicada es mucha, o lo que es lo mismo, si las diferencias entre los datos y las predicciones según el modelo son pequeñas.



::: {.callout-caution title="Repaso"}
Es conveniente repasar el tema de *Análisis de la Varianza* estudiado en la asignatura de Inferencia.
:::

#### Variabilidad total 

La variabilidad total de$Y$ se mide mediante la **Suma Total de los Cuadrados (SST)**:  
$$
SST = \sum_{i=1}^n (Y_i - \bar{Y})^2,
$$  
donde $\bar{Y}$ es la media de $Y$. $SST$ refleja la dispersión general de $Y$ respecto a su media.

**Descomposición de la variabilidad**

El modelo de regresión lineal permite descomponer$SST$ en dos componentes principales:  
$$
SST = SSR + SSE,
$$  
donde:

  - $SSR$ (Suma de los Cuadrados del Modelo): Representa la variabilidad explicada por la regresión, es decir, la parte de $Y$ que se puede predecir a partir de $X$:  
 $$
  SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2,
 $$
  donde $\hat{Y}_i$ es el valor predicho por el modelo para el $i$-ésimo dato.

  - $SSE$ (Suma de los Cuadrados de los Errores): Como hemos visto al inicio de esta sección, representa la variabilidad no explicada por el modelo, es decir, la dispersión de los valores observados respecto a los valores predichos:  
 $$
  SSE = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2.
 $$


#### Tabla ANOVA

El análisis de varianza organiza la descomposición de la varianza en una tabla, donde cada componente se asocia con sus grados de libertad ($df$), suma de cuadrados ($SS$), media cuadrática ($MS$) y el estadístico $F$:

| Fuente       |$df$          |$SS$           |$MS = SS/df$       | Estadístico$F$         |
|--------------|-----------------|------------------|----------------------|---------------------------|
| Regresión    | 1               |$SSR$          |$MSR = SSR/1$      |$F = MSR/MSE$           |
| Error        |$n-2$         |$SSE$          |$MSE = SSE/(n-2)$  |                           |
| Total        |$n-1$         |$SST$          |                      |                           |

#### Prueba de significancia global

Para evaluar si $X$ tiene un efecto significativo sobre $Y$, se utiliza el estadístico $F$:  
$$
F = \frac{MSR}{MSE}.
$$  

  - Bajo la hipótesis nula ($H_0: \beta_1 = 0$), $F$ sigue una distribución $F$ con 1 y $n-2$ grados de libertad.
  - Si el valor $p$ asociado al estadístico $F$ es pequeño ($p < \alpha$), se rechaza $H_0$, indicando que $X$ tiene un efecto significativo sobre $Y$.

El análisis ANOVA permite responder preguntas clave: 

  - ¿Qué proporción de la variabilidad de $Y$ es explicada por $X$?  
  - ¿Es significativa esta relación desde el punto de vista estadístico?  

En el contexto de la regresión lineal simple, esta herramienta no solo cuantifica el ajuste del modelo, sino que también valida su relevancia estadística.


::: {.callout-tip title="Ejemplo" collapse="true"}
A continuación se presenta la tabla ANOVA correspondiente al modelo de horas de estudio y calificaciones, previamente entrenado.

```{r anova}
# Ajustar el modelo de regresión lineal
modelo <- lm(calificaciones ~ tiempo_estudio)

# Obtener y mostrar la tabla ANOVA
tabla_anova <- anova(modelo)
cat("Tabla ANOVA:\n")
print(tabla_anova)

```
:::


### Bondad del ajuste: coeficiente de determinación

El coeficiente de determinación ($R^2$) mide qué proporción de la
variabilidad total en $Y$ es explicada por $X$ a través del modelo:

$$
R^2 = 1 - \frac{\text{Suma de los Cuadrados de los Residuos (SSE)}}{\text{Suma Total de los Cuadrados (SST)}} = \frac{\text{Suma de los Cuadrados del Modelo (SSR)}}{\text{Suma Total de los Cuadrados (SST)}}.
$$

Donde:

-   $\text{SST} = \sum_{i=1}^n (Y_i - \bar{Y})^2$: Variabilidad total en
    $Y$.\
-   $\text{SSR} = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$: Variabilidad
    explicada por el modelo.\
-   $\text{SSE} = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$: Variabilidad no
    explicada.

Un $R^2$ cercano a 1 indica que el modelo ajusta bien los datos,
mientras que un $R^2$ cercano a 0 indica un ajuste pobre.




::: {.callout-tip title="Ejemplo" collapse="true"}
En el resumen del modelo anterior hemos obtenido el siguiente valor para el coeficiente de determinación $R^2=0.8069$. ¿Cómo evaluas el ajuste obtenido en este modelo?
:::

::: {.callout-note title="Observaciones" collapse="false"}
  -  $R^2$ debe ser interpretado con cautela, ya que resultará con frecuencia grande a pesar de que la relación entre $X$ e $Y$ no sea lineal.
  - Ası́ por ejemplo, la magnitud d $R^2$2 depende del rango de variabilidad de la variable explicativa, $X$. Siendo el modelo de regresión adecuado, la magnitud de $R^2$ aumenta (o disminuye) cuando lo hace la dispersión de $X$.
  - Además $R^2$ podrı́a ser un valor muy pequeño debido a que el rango de variación de $X$ es demasiado pequeño e impide que se detecte la relación con $Y$.
:::

### Diagnóstico del modelo

El diagnóstico del modelo de regresión es un paso esencial para evaluar si los supuestos subyacentes se cumplen y garantizar la validez de las inferencias. El análisis de residuos proporciona información clave sobre la calidad del ajuste del modelo y la adecuación de los datos a los supuestos de la regresión lineal.

El análisis de residuos ayuda a verificar los siguientes supuestos del modelo:

  - Linealidad: La relación entre $X$ e $Y$es lineal.
  - Independencia: Los residuos son independientes entre sí.
  - Homocedasticidad: Los residuos tienen varianza constante.
  - Normalidad: Los residuos se distribuyen de manera aproximadamente normal.

Una vez ajustado el modelo, hemos de detectar desviaciones de las hipótesis: proceder al diagnóstico del modelo. El análisis de los residuos nos permitirá identificar deficiencias en la verificación de estas hipótesis, ası́ como observaciones anómalas o influyentes.

Aquí tienes el desarrollo de la sección **"Diagnóstico del modelo. Análisis de residuos"**:

---

## **Diagnóstico del Modelo. Análisis de Residuos**

El diagnóstico del modelo de regresión es un paso esencial para evaluar si los supuestos subyacentes se cumplen y garantizar la validez de las inferencias. El análisis de residuos proporciona información clave sobre la calidad del ajuste del modelo y la adecuación de los datos a los supuestos de la regresión lineal.

Recordemos que los residuos del modelo ($e_i$) son las diferencias entre los valores observados ($Y_i$) y los valores predichos ($\hat{Y}_i$) por el modelo:
$$
e_i = Y_i - \hat{Y}_i.
$$
Representan la parte de la variabilidad en $Y$ no explicada por el modelo.

En ocasiones, es preferible trabakar con los **residuos estandarizados**, que tienen media cero y varianza aproximadamente unidad:
$$
d_i = \frac{e_i}{\sqrt{MSE}},  \hspace{0.5cm} i=1,\ldots,n.
$$
Otro tipo de residuos habitual es el de los llamados **residuos estudentizados**:
$$
d_i = \frac{e_i}{\sqrt{MSE \left ( 1-\frac{1}{n}-\frac{(X_i-\bar{X})^2}{S_xx} \right )}}, \hspace{0.5cm} i=1,\ldots,n.
$$

Estos residuos son preferibles a los estandarizados cuando $n$ es pequeño. 

#### Herramientas para el análisis de los residuos



