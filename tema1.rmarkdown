# Modelo de regresión lineal: simple y múltiple {#sec-tema1}

La regresión lineal constituye uno de los pilares fundamentales de la modelización estadística y es ampliamente utilizada en múltiples disciplinas para analizar relaciones entre variables. Este modelo permite explorar cómo una o varias variables explicativas (independientes) influyen sobre una variable respuesta (dependiente), proporcionando no solo descripciones útiles sino también herramientas para la predicción y la inferencia.

El **modelo de regresión lineal simple** se centra en analizar la relación entre una única variable explicativa y una respuesta, mientras que el **modelo de regresión lineal múltiple** extiende este concepto al considerar múltiples variables explicativas, permitiendo capturar relaciones más complejas y realistas. Ambos modelos comparten principios fundamentales, como el ajuste de una recta mediante el criterio de mínimos cuadrados y la interpretación de sus parámetros, pero difieren en su alcance y en los retos que presentan.

En este capítulo, abordaremos los fundamentos de la regresión lineal simple, incluyendo sus suposiciones clave, el procedimiento de estimación de parámetros y el uso de herramientas de diagnóstico. A través de ejemplos prácticos, como los estudios clásicos de Galton sobre la herencia de estaturas o el análisis de datos de inversión en publicidad, se ilustrará el poder descriptivo y predictivo de este modelo.  

Este tema también prepara las bases para comprender el modelo de regresión lineal múltiple, presentado como una generalización del simple, y para abordar las complejidades adicionales que surgen, como la colinealidad entre variables explicativas y la selección de modelos óptimos.  

La comprensión y aplicación correcta de estos modelos es esencial no solo para su utilidad en contextos prácticos, sino también porque representan el punto de partida para técnicas más avanzadas en la ciencia de datos y el aprendizaje automático.  

::: {.callout-important title="Objetivos"}
Los siguiente objetivos buscan sentar las bases teóricas y prácticas necesarias para entender y aplicar modelos de regresión, preparando al estudiante para explorar modelos más complejos en los temas posteriores.


1. **Introducción a la Modelización estadística**:  
   - Comprender el proceso de modelización estadística, desde la definición de objetivos y variables hasta el ajuste y evaluación del modelo.

2. **Relación lineal y correlación**:  
   - Introducir conceptos como la correlación y la relación lineal entre variables.  
   - Analizar cómo identificar y medir la fuerza de la relación entre dos variables.

3. **Comprensión del modelo de regresión lineal simple**:  
   - Describir la estructura del modelo de regresión lineal simple y entender su formulación matemática.  
   - Interpretar los parámetros del modelo (intercepto y pendiente) y su significado en un contexto práctico.  

4. **Procedimiento de inferencia estadística**:  
   - Estimar los coeficientes del modelo mediante el método de mínimos cuadrados.  
   - Realizar inferencias sobre los parámetros, incluyendo contrastes de hipótesis y predicciones.

5. **Diagnóstico del modelo**:  
   - Introducir herramientas para diagnosticar la adecuación del modelo ajustado.  
   - Evaluar la validez de los supuestos del modelo, como la linealidad, homocedasticidad e independencia de errores.

6. **Aplicación práctica**:  
   - Implementar el modelo en contextos reales, como el análisis de la influencia de inversiones en ganancias o la herencia de características biológicas (ejemplo de Galton).  
   - Visualizar y analizar gráficamente las relaciones para facilitar la interpretación de los resultados.
:::

## Modelización estadística

La modelización estadística es un proceso estructurado que permite analizar y describir relaciones entre variables mediante modelos matemáticos. Este enfoque es fundamental en la estadística aplicada y proporciona herramientas para interpretar datos, realizar predicciones y tomar decisiones fundamentadas. A continuación, se describen los pasos clave de este proceso. Este enfoque sistemático asegura que el modelo ajustado sea robusto, interpretable y útil para realizar predicciones confiables. La revisión continua y el ajuste basado en evidencia permiten capturar la complejidad de los datos de manera efectiva.

### Contextualización del problema

El primer paso en la modelización estadística es definir el problema que se busca analizar. Esto incluye identificar el contexto, establecer objetivos claros y determinar las variables involucradas.


::: {.callout-tip title="Ejemplo"  collapse="true"}

- **Problema:** Investigar si existe una relación positiva entre el tiempo dedicado al estudio semanal y el promedio de calificaciones de los estudiantes universitarios.  
- **Variables:**  
  - Variable explicativa (independiente): Tiempo de estudio semanal (en horas).  
  - Variable respuesta (dependiente): Promedio de calificaciones al final del semestre (en una escala de 0 a 10).  
- **Objetivo:** Determinar si los estudiantes que dedican más horas al estudio semanalmente obtienen mejores calificaciones en promedio.  



Emplearemos datos simulados para este ejemplo:





```{r datos, warning=FALSE}
# Establecer la semilla para reproducibilidad
set.seed(123)

# Número de estudiantes
n <- 100

# Generar tiempo de estudio (en horas) como una variable independiente
tiempo_estudio <- round(runif(n, min = 5, max = 40), 1)

# Relación lineal entre tiempo de estudio y calificaciones (con ruido)
beta_0 <- 5  # Intercepto
beta_1 <- 0.1  # Pendiente (efecto del tiempo de estudio)
sigma <- 0.5  # Varianza del ruido

# Esta será la relación que deseamos "descubrir".
calificaciones <- round(beta_0 + beta_1 * tiempo_estudio + rnorm(n, mean = 0, sd = sigma), 2)

# Crear un data frame con los datos generados
datos <- data.frame(Tiempo_Estudio = tiempo_estudio, Calificaciones = calificaciones)

# Visualizar los primeros registros
head(datos)

```





:::
### Inspección gráfica e identificación de tendencias

Antes de ajustar un modelo, es esencial realizar una inspección visual de los datos. Los gráficos de dispersión son una herramienta útil para observar tendencias, relaciones o patrones entre las variables.

::: {.callout-tip title="Ejemplo"  collapse="true"}
En el ejemplo, representamos el tiempo de estudio frente a las calificaciones. De este modo podemos analizar si los puntos siguen un patrón lineal o si muestran comportamientos más complejos.






```{r grafico1, warning=FALSE}

# Graficar los datos de estudio generados anteriormente
plot(datos$Tiempo_Estudio, datos$Calificaciones,
     main = "Relación entre Tiempo de Estudio y Calificaciones",
     xlab = "Tiempo de Estudio (horas/semana)",
     ylab = "Calificaciones (promedio)",
     pch = 19, col = "blue")

```





:::

### Propuesta y ajuste del modelo
Con base en las observaciones previas, se propone un modelo estadístico que relacione las variables. En el caso de relaciones lineales, la ecuación típica es:  

$$
\text{Respuesta} = \beta_0 + \beta_1 (\text{Variable explicativa}) + \varepsilon,
$$

donde $\beta_0$ es la constante (o intercepto), $\beta_1$ es la pendiente, y $\varepsilon$ es el término de error aleatorio.  

Fíjate que la expresión anterior concuerda con el siguiente principio fundamental del análisis de datos:

$$DATOS = MODELO + ERROR$$

-   Los **datos** representan la realidad (procesos de negocios, clientes, productos, actividades, fenómenos físicos, etc.) que se quiere comprender, predecir o mejorar.

-   El **modelo** es una representación **simplificada** de la realidad que proponemos para describirla e interpretarla más fácilmente.

-   El **error** refleja la diferencia entre nuestra representación simplificada de la realidad (el modelo) y los datos que relamente describen esa realidad de forma precisa.


::: {.callout-tip title="Ejemplo" collapse="true"}
  - Propuesta del modelo:  
  $$
    \text{Calificaciones} = \beta_0 + \beta_1 (\text{Tiempo de Estudio}).
    $$
  - **Ajuste:** Calcular los valores de \( \beta_0 \) y \( \beta_1 \) mediante el método de mínimos cuadrados. Este método busca minimizar la suma de los errores cuadrados entre los valores observados y los predichos por el modelo.  







```{r ajuste1, warning=FALSE}

# Ajustar un modelo de regresión lineal
modelo <- lm(Calificaciones ~ Tiempo_Estudio, data = datos)
summary(modelo)

# Graficar los datos de estudio y su recta de regresión lineal
plot(datos$Tiempo_Estudio, datos$Calificaciones,
     main = "Relación entre Tiempo de Estudio y Calificaciones",
     xlab = "Tiempo de Estudio (horas/semana)",
     ylab = "Calificaciones (promedio)",
     pch = 19, col = "blue")
abline(lm(Calificaciones ~ Tiempo_Estudio, data = datos), col = "red", lwd = 2)
```






A la vista de los estimadores de los parámetros del modelo, tenemos la siguiente interpretación:

- Se produce un incremento de la calificiación de $0.1$ (aproximadamente) por cada incremento de $1$ hora en el tiempo de estudio. O bien, por cada $10$ horas de estudio, se incrementa la nota en $1$ punto.

- En ocasiones es complicado interpretar el valor de $\beta_0$. En este caso, corresponde a la calificación de los estudiantes que no dedican ningún tiempo de estudio. En este caso sería $5$ (aproximadamente).
:::


### Revisión y diagnóstico del modelo

Una vez ajustado el modelo, es crucial evaluar su calidad. Esto implica analizar si los supuestos del modelo se cumplen, como la linealidad, la homocedasticidad y la independencia de los errores.

Deberemos realizar las siguientes tareas:

- Comparar los valores predichos por el modelo con los datos observados para verificar su ajuste.

- Examinar los residuos (errores) para identificar posibles problemas, como tendencias no capturadas o varianzas no constantes.

### Reajuste del modelo

Si el diagnóstico revela deficiencias en el modelo ajustado, se plantean modificaciones. Estas pueden incluir:

- Transformaciones de variables para mejorar la linealidad.

- Introducción de términos adicionales, como variables cuadráticas o interacciones.

- Cambios a modelos no lineales si el comportamiento de los datos lo requiere.

::: {.callout-tip title="Estudios de Galton sobre estatura"}
Los estudios de Sir Francis Galton sobre la estatura son un ejemplo clásico en estadística y forman parte de la historia de la regresión lineal. Galton, un polímata británico del siglo XIX, investigó la herencia biológica y publicó en 1889 su libro *Natural Inheritance*, donde analizó datos sobre la relación entre las estaturas de padres e hijos. Estos estudios no solo sentaron las bases para la regresión lineal, sino que también ayudaron a formalizar conceptos clave en estadística, haciendo de Galton una figura central en su desarrollo. 

**Contexto y propósito**

Galton estaba interesado en cómo las características físicas, como la estatura, se transmiten de padres a hijos. Su objetivo era cuantificar esta relación y establecer patrones de herencia. En particular, buscó responder si los hijos de padres altos tienden a ser más altos y si los de padres bajos tienden a ser más bajos.

**Datos recopilados**

- Galton recopiló datos sobre las estaturas de **928 hijos** y sus respectivos **padres**. 
- Las medidas fueron expresadas en pulgadas (1 pulgada = 2.54 cm).  
- En sus análisis, utilizó el promedio de las estaturas de ambos padres, conocido como **estatura media parental**, para compararlo con la estatura de los hijos.

**Principales hallazgos**

1. **Relación lineal entre padres e hijos:**  
   Galton observó que existe una relación positiva entre la estatura de los padres y la de los hijos. Los padres altos tienden a tener hijos altos, y los padres bajos tienden a tener hijos bajos. Esta relación puede modelarse con una línea recta, lo que inspiró la formulación de la regresión lineal.

2. **Regresión a la media:**  
   - Aunque los hijos de padres altos son, en promedio, más altos que el promedio general de la población, también tienden a ser **menos altos que sus padres**.  
   - De manera similar, los hijos de padres bajos son más bajos que el promedio general, pero suelen ser **menos bajos que sus padres**.  
   - Este fenómeno, que Galton llamó "regresión a la media", ocurre porque las características extremas tienden a suavizarse en la siguiente generación debido a la influencia de múltiples factores genéticos y ambientales.

3. **Ecuación de la recta de regresión:**  
   Galton ajustó una recta para describir la relación entre la estatura media parental ($X$) y la estatura de los hijos ($Y$):
$$
   Y = \beta_0 + \beta_1 X
$$
   Donde:
   - $\beta_0$: Intercepto, representa la estatura promedio de los hijos cuando la estatura parental es promedio.
   - $\beta_1$: Pendiente, indica cómo cambia la estatura de los hijos por cada unidad de cambio en la estatura media parental.

**Importancia en la Estadística**

1. **Regresión lineal:**  
   Este estudio introdujo el concepto de **recta de regresión**, que describe cómo varía la media de una variable dependiente en función de una variable independiente.

2. **Correlación:**  
   Galton también estudió el grado de relación entre variables, precursor del concepto de **coeficiente de correlación** desarrollado posteriormente por Karl Pearson, un discípulo suyo.

3. **Regresión a la media:**  
   El término y la idea detrás de "regresión a la media" surgieron de estos estudios y son hoy fundamentales en estadística y genética.

**Ejemplo Gráfico**

Galton representó sus datos en gráficos de dispersión, mostrando cómo los puntos (pares de estatura media parental y estatura de los hijos) se agrupan alrededor de la recta de regresión, ilustrando la tendencia general de la relación.


¡Claro! Aquí tienes el código en R para crear un gráfico con los datos analizados por Galton, añadiendo la recta de regresión y su ecuación:






```{r galtonn}
# Instalar y cargar los paquetes necesarios
install.packages("ggplot2")
install.packages("HistData")
library(ggplot2)
library(HistData)

# Cargar los datos de Galton
data("GaltonFamilies")
galton_data <- GaltonFamilies

# Crear el modelo de regresión lineal
modelo <- lm(childHeight ~ parentHeight, data = galton_data)

# Crear el gráfico con ggplot2
grafico <- ggplot(galton_data, aes(x = parentHeight, y = childHeight)) +
  geom_point() +  # Añadir puntos de datos
  geom_smooth(method = "lm", col = "red") +  # Añadir la recta de regresión
  labs(title = "Altura de Padres e Hijos (Datos de Galton)",
       x = "Altura de los Padres",
       y = "Altura de los Hijos") +
  annotate("text", x = 70, y = 75, label = paste("y =", round(coef(modelo)[1], 2), "+", round(coef(modelo)[2], 2), "x"), color = "red")

# Mostrar el gráfico
print(grafico)
```






:::

::: callout-caution
## Ejercicio

Dejamos como ejercicio para el alumno la interpretación del resultado del test.
:::

